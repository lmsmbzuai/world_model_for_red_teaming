"""
* File: ./environment/model_manager.py
* Author: Loic Martins
* Date: 2025-11-20
* Description: Orchestrate the model(s) configurations for the agents using Ollama API.
"""

# Import External Libraries
from typing import Any

import requests

# Import Local Modules
from experiment.config import ExperimentConfig


class ModelManager:
    def __init__(self, config: ExperimentConfig, model_name: str) -> None:
        """
        Initialize Ollama model manager.

        Prerequisites:
        1. Install Ollama: https://ollama.com
        2. Pull model: ollama pull llama3.1:8b
        3. Pull model: ollama pull qwen2.5:7b-instruct
        3. Start server: ollama serve

        Args:
            config (ExperimentConfig): Specific configurations for the environment.
            model_name (str): Ollama model name (e.g., "llama3.1:8b")

        Returns:
            None.
        """

        self.config: ExperimentConfig = config
        self.model_name: str = model_name
        self.api_url: str = f"{self.config.base_url}/api/chat"

        # Test connection
        try:
            response = requests.get(f"{self.config.base_url}/api/tags")
            response.raise_for_status()
            print(
                f"====Step 3.1 - Travel Planning agentic environment - Connected to Ollama - Model: {self.model_name}"
            )
        except Exception as e:
            print(f"Cannot connect to Ollama: {e}")
            print("Make sure Ollama is running: ollama serve")

    def generate(
        self,
        system_prompt: str,
        conversation_history: list[dict[str, str]] | list[str],
        temperature: float | None = None,
        top_p: int | None = None,
    ) -> str:
        """
        Generate response using Ollama API.

        Args:
            system_prompt (str): System prompt defining agent behavior.
            conversation_history (list[dict[str, str]] | list[str]): The history of the conversation for the specific agent:
                [{"role": "user", "content": "..."}, ...]
            temperature (float): Specific parameter for the model --controls randomness in token selection
            top_p (int): Specific parameter for the model --limits choices to the smallest set of tokens whose cumulative probability â‰¤ p

        Returns:
            Generated response (str): Message content generated by the model.
        """
        # Step 1: Prepare messages with system prompt
        formatted_messages = [
            {"role": "system", "content": system_prompt}
        ] + conversation_history

        # Step 2: Send Data to the Ollama API
        payload = {
            "model": self.model_name,
            "messages": formatted_messages,
            "stream": False,
            "options": {
                "temperature": 0.7 if temperature is None else temperature,
                "top_p": 0.9 if not top_p else top_p,
            },
            "seed": self.config.seed,
        }
        try:
            response = requests.post(self.api_url, json=payload, timeout=60)
            response.raise_for_status()
            result: dict[str, Any] = response.json()
            return result["message"]["content"]
        except requests.exceptions.Timeout:
            return "Error: Request timed out"
        except requests.exceptions.RequestException as e:
            return f"Error: API request failed - {str(e)}"
        except KeyError as e:
            return f"Error: Unexpected response format - {str(e)}"
